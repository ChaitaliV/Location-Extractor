# -*- coding: utf-8 -*-
"""class_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ChaitaliV/Splurge/blob/main/location_extractor/class_final.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import numpy as np
import os
import openai
import re
from transformers import BertTokenizer, BertForTokenClassification
from keras_preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import requests

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = torch.load(r'/content/drive/MyDrive/BERT_text.pt')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
base_url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
api_key = "AIzaSyCU7kaDfhZIM4bbJVujlGlhdXphUPke1yY"
engine = "text-davinci-002"
MAX_LEN = 512

class LocationExtractor:
    def __init__(self, sent, city):
        self.device = device
        self.tokenizer = tokenizer
        self.model = model
        self.model.to(self.device)
        self.city = city
        self.sent = sent
        self.categories = ['Restaurant', 'Cafe', 'House', 'Barbecue', 'Bar', 'Pub', 'Palace', 'Kitchen', 'Club', 'Bakery', 'Shop', 'Room', 'Shack', 'Garden', 'Factory', 'Queen', 'Street','Castle', 'Mall', 'Park', 'Palace', 'Temple', 'Masjid', 'Dargah', 'Avenue', 'Gallery', 'Museum', 'Garden', 'Hotel', 'Lake', 'Fort', 'Beach', 'Mandir', 'Hill', 'Bhavan', 'Nation', 'World', 'Center','Pavilion', 'Bistro']

    def clean_string(self, lst, sent):
        l = []
        for ele in lst:
            if ele == ' ' or ele == '':
                pass
            else:
                l.append(ele)
        return l

    def add_suffix(self, p, sent):
        match = re.search(p, sent)
        if match:
            for category in self.categories:
                if match.end() < len(sent) and sent[match.end():].lower().startswith(category.lower()):
                    l = match.group() + '' + category
                    return l
                    break
            return match.group()
        else:
            return p
        
    def add_next_word(self, p, sent):
        match = re.search(p, sent)
        if match:
            s = sent[match.end():].split(' ')
            try: 
              if (s[0] == ''):
                p = p + ' ' + s[1]
              elseif (s[0] == ' '):
                p = p + ' ' + s[1]
              else:
                p = p+' '+s[0]
            except:
              pass
        return p

    def extract_location(self, sent):
        true_label = []
        tokenized_sentence = self.tokenizer.encode(sent)
        input_ids = torch.tensor([tokenized_sentence]).to(self.device)
        with torch.no_grad():
            output = self.model(input_ids)
        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
        new_tokens, new_labels = [], []
        for token, label_idx in zip(tokens, label_indices[0]):
            if token.startswith("##"):
                new_tokens[-1] = new_tokens[-1] + token[2:]
            else:
                new_labels.append(label_idx)
                new_tokens.append(token)
        for token, label in zip(new_tokens, new_labels):
            if (label == 3):
                true_label.append(token)
            else:
                true_label.append('#')
        label = " ".join(true_label)
        label = label.replace(" ' s","'s")
        lst = label.split('#')
        p = self.clean_string(lst, sent)
        final_lst = []
        for ele in p:
            final_lst.append(self.add_suffix(ele, sent))
        list_2 = []
        for ele in final_lst:
            list_2.append(self.add_next_word(ele,sent))
        return list(filter(None, list_2))

    def multiline_data(self, text):
        text = text.replace('\n','.')
        names = []
        lst = text.split(".")
        for sent in lst:
            names.append(self.extract_location(sent))
        data = list(filter(None, names))
        l = []
        for place_list in data:
            for i in place_list:
                l.append(i + ', ' + self.city)
        return l

class PlaceFinder:
    def __init__(self, data):
        self.base_url = base_url
        self.api_key = api_key
        self.data = data

    def fetch_place_details(self,place_name):
      # Set up the parameters for the API request
      global result
      params = {
          'key': self.api_key,
          'input': place_name,
          'inputtype': 'textquery',
          'fields': 'place_id,name,formatted_address,rating,opening_hours,geometry,photos,types'
      }

      # Send the API request
      response = requests.get(self.base_url, params=params).json()

      # Check if the response contains any results
      if response['status'] == 'ZERO_RESULTS':
          result = ''
      else:
          result = response['candidates'][0]
          # Get the place ID of the first result (assuming it is the correct restaurant)
          place_id = response['candidates'][0]['place_id']

          # Make a request to the Places Details API to fetch the phone number
          details_url = f'https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&fields=international_phone_number&key={api_key}'
          details_response = requests.get(details_url).json()

          # Extract the relevant details from the API response
          details = {}
          details['Name']= result['name']
          details['Address']= result['formatted_address']
          details['Rating'] = result.get('rating', 'N/A')
          details['Opening Hours'] =  result.get('opening_hours', 'N/A')
          details['Location']= result['geometry']['location']
          details['Types'] = result.get('types', 'N/A')[0]

          details['Phone'] = details_response['result'].get('international_phone_number', 'N/A')
          photos = []
          photo_references = result.get('photos', [])
          for photo_reference in photo_references:
              photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_reference['photo_reference']}&key={api_key}"
              photos.append(photo_url)
          details['Photos'] = photos

          return details

    
    def get_places_data(self):
        data = []
        for ele in self.data:
            data.append(self.fetch_place_details(ele))
        return data
    
    
   
class PlaceDescriptionGenerator:
    def __init__(self, openai_key):
        self.openai_key = openai_key
        self.engine = engine
        openai.api_key = openai_key
        
        
    def get_description(self,place_name):
      # Set the prompt for the API
      prompt = """Get a 150 characters description about the place """ + place_name
     
      # Generate a response to the prompt
      response = openai.Completion.create(
          engine=self.engine,
          prompt=prompt,
          max_tokens=1024,
          n=1,
          stop=None,
          temperature=0.7,
      )

      # Extract the generated text from the response
      message = response.choices[0].text.strip()
      
      # Split the message into the description and tags
      place_tokens = place_name.split(',')
      for token in place_tokens:
          message1 = message.replace(token, '')
      return message1, message
    
    def get_tags(self,description):
      prompt = """
      Generate only three short tags about the place from the """+description
      # Generate a response to the prompt
      response = openai.Completion.create(
          engine=engine,
          prompt=prompt,
          max_tokens=32,
          n=1,
          stop=None,
          temperature=0.7,
      )

      # Extract the generated text from the response
      message = response.choices[0].text.strip()
      words_to_remove = ['Tag','Tags','input','tags','tag','Input','var','Place','place','type','Type','CODE','enter','YOUR  HERE','Output','here','list of','three','append']
      message = message.replace('\n',',')
      clean_text = re.sub("[^A-Za-z,']+", ' ', message)
      for word in words_to_remove:
          clean_text = clean_text.replace(word,'')
      return clean_text
      
    def list_tag_data(self,data):
      all_data = []
      for place in data:
        info = {}
        des = self.get_description(place)
        info['Tags'] = self.get_tags(des)
        info['Description'] = des
        all_data.append(info)
      return all_data
