# -*- coding: utf-8 -*-
"""class_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ChaitaliV/Splurge/blob/main/location_extractor/class_final.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install openai
!pip install pytorch-pretrained-bert pytorch-nlp
!pip install -q transformers
!pip install Keras-Preprocessing

import torch
import torch.nn as nn
import numpy as np
import re
from transformers import BertTokenizer, BertForTokenClassification
from keras_preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = torch.load(r'/content/drive/MyDrive/BERT_text.pt')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MAX_LEN = 512

class LocationExtractor:
    def __init__(self, sent):
        self.device = device
        self.tokenizer = tokenizer
        self.model = model
        self.model.to(self.device)
        self.sent = sent
        self.categories = ['Restaurant', 'Cafe', 'House', 'Barbecue', 'Bar', 'Pub', 'Palace', 'Kitchen', 'Club', 'Bakery', 'Shop', 'Room', 'Shack', 'Garden', 'Factory', 'Queen']

    def clean_string(self, lst, sent):
        l = []
        for ele in lst:
            if ele == ' ' or ele == '':
                pass
            else:
                l.append(ele)
        return l

    def add_suffix(self, p, sent):
        match = re.search(p, sent)
        if match:
            for category in self.categories:
                if match.end() < len(sent) and sent[match.end():].lower().startswith(category.lower()):
                    l = match.group() + '' + category
                    return l
                    break
            return match.group()
        else:
            return p

    def extract_location(self, sent):
        true_label = []
        tokenized_sentence = self.tokenizer.encode(sent)
        input_ids = torch.tensor([tokenized_sentence]).to(self.device)
        with torch.no_grad():
            output = self.model(input_ids)
        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
        new_tokens, new_labels = [], []
        for token, label_idx in zip(tokens, label_indices[0]):
            if token.startswith("##"):
                new_tokens[-1] = new_tokens[-1] + token[2:]
            else:
                new_labels.append(label_idx)
                new_tokens.append(token)
        for token, label in zip(new_tokens, new_labels):
            if (label == 3):
                true_label.append(token)
            else:
                true_label.append('#')
        label = " ".join(true_label)
        label = label.replace(" ' s","'s")
        lst = label.split('#')
        p = self.clean_string(lst, sent)
        final_lst = []
        for ele in p:
            final_lst.append(self.add_suffix(ele, sent))
        return list(filter(None, final_lst))

    def multiline_data(self, text):
        names = []
        lst = text.split(".")
        for sent in lst:
            names.append(self.extract_location(sent))
        return list(filter(None, names))

sentence = "I went to Paris and visited the Eiffel Tower."
location_extractor = LocationExtractor(sentence)
location_extractor.extract_location(sentence)

