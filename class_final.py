# -*- coding: utf-8 -*-
"""class_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ChaitaliV/Splurge/blob/main/location_extractor/class_final.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import numpy as np
import os
import openai
import re
from transformers import BertTokenizer, BertForTokenClassification
from keras_preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import requests

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = torch.load(r'/content/drive/MyDrive/BERT_text.pt')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
base_url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
api_key = "AIzaSyCU7kaDfhZIM4bbJVujlGlhdXphUPke1yY"
engine = "text-davinci-002"
MAX_LEN = 512

class LocationExtractor:
    def __init__(self, sent):
        self.device = device
        self.tokenizer = tokenizer
        self.model = model
        self.model.to(self.device)
        self.sent = sent
        self.categories = ['Restaurant', 'Cafe', 'House', 'Barbecue', 'Bar', 'Pub', 'Palace', 'Kitchen', 'Club', 'Bakery', 'Shop', 'Room', 'Shack', 'Garden', 'Factory', 'Queen']

    def clean_string(self, lst, sent):
        l = []
        for ele in lst:
            if ele == ' ' or ele == '':
                pass
            else:
                l.append(ele)
        return l

    def add_suffix(self, p, sent):
        match = re.search(p, sent)
        if match:
            for category in self.categories:
                if match.end() < len(sent) and sent[match.end():].lower().startswith(category.lower()):
                    l = match.group() + '' + category
                    return l
                    break
            return match.group()
        else:
            return p

    def extract_location(self, sent):
        true_label = []
        tokenized_sentence = self.tokenizer.encode(sent)
        input_ids = torch.tensor([tokenized_sentence]).to(self.device)
        with torch.no_grad():
            output = self.model(input_ids)
        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])
        new_tokens, new_labels = [], []
        for token, label_idx in zip(tokens, label_indices[0]):
            if token.startswith("##"):
                new_tokens[-1] = new_tokens[-1] + token[2:]
            else:
                new_labels.append(label_idx)
                new_tokens.append(token)
        for token, label in zip(new_tokens, new_labels):
            if (label == 3):
                true_label.append(token)
            else:
                true_label.append('#')
        label = " ".join(true_label)
        label = label.replace(" ' s","'s")
        lst = label.split('#')
        p = self.clean_string(lst, sent)
        final_lst = []
        for ele in p:
            final_lst.append(self.add_suffix(ele, sent))
        return list(filter(None, final_lst))

    def multiline_data(self, text):
        names = []
        lst = text.split(".")
        for sent in lst:
            names.append(self.extract_location(sent))
        data = list(filter(None, names))
        l = []
        for place_list in data:
            for i in place_list:
                l.append(i + ', ' + self.city)
        return l

class PlaceFinder:
    def __init__(self, data):
        self.base_url = base_url
        self.api_key = api_key
        self.data = data
        
    def fetch_place_details(self, place):
        # Set up the parameters for the API request
        params = {
            'key': self.api_key,
            'input': place,
            'inputtype': 'textquery',
            'fields': 'name,formatted_address,rating,opening_hours,geometry'
        }
        
        # Send the API request
        response = requests.get(self.base_url, params=params).json()

        # Check if the response contains any results
        if response['status'] == 'ZERO_RESULTS':
            print('No results found.')
        else:
            # Get the details of the first result (assuming it is the correct restaurant)
            try: 
                result = response['candidates'][0]
            except:
                result = response
                
        details = {}
        details['Name'] = result['name']
        details['Address'] = result['formatted_address']
        details['Rating'] = result.get('rating', 'N/A')
        details['Opening Hours'] = result.get('opening_hours', 'N/A')
        details['Location'] = result['geometry']['location']
        
        return details
    
    def get_places_data(self):
        data = []
        for ele in self.data:
            data.append(self.fetch_place_details(ele))
        return data

